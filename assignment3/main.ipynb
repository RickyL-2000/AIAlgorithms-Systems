{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole 强化学习实验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 实验介绍\n",
    "\n",
    "### 1.1 实验背景\n",
    "\n",
    "小车和杆子套在一个光滑无摩擦的轨道上。\n",
    "杆子只要有倾斜，小车就会滑动。\n",
    "目标是，通过对小车施加力，保持杆子平衡\n",
    "详见 OpenAI Gym  https://github.com/openai/gym\n",
    "    \n",
    "### 1.2 实验要求\n",
    "\n",
    "补全代码\n",
    "\n",
    "任务1：使用 Actor –Critic 算法，在环境中进行训练 (共70分)\n",
    "\n",
    "任务2： 假设RL因果图如下 S 为 state， a 为action ， R为reward/value function,  R 的产生可能取决于state 和 action。 在1的强化学习方法中，设计一种算法，来估计出P(R|do(A)).      （共30分）\n",
    "\n",
    "<div class='insertContainerBox column'>\n",
    "<div class='insertItem' align=center><img src=\"https://imgbed.momodel.cn/因果图.jpg\" width=\"200px\"/></div>\n",
    "</div>\n",
    "\n",
    "方案 1：计算Advantage function，并用其进行训练，Advantage function 相当于反事实的action-value function。（15分）  \n",
    "方案 2：计算$P(R|do(A))=\\sum_{S}P(R|A,S)P(S)$     (30分)\n",
    "\n",
    "### 1.3 代码编写说明\n",
    "\n",
    "所需要编写的代码地方均有中文注释\n",
    "任务1：补充网络模型 class AC1 以及训练过程相关代码即可。\n",
    "\n",
    "任务2：补充网络模型 class AC2 以及修改训练过程相关代码即可。\n",
    "\n",
    "任务2-1 主要考察 Advantage 函数的计算方式 \n",
    "任务2-2 需要修改 policy_task2.py 模型结构，以及 Q 函数的计算过程\n",
    "（任务2-2可行方法示例，设计$M$个相同的network，在每个time-step，对同一个state，同时产生M个表征， 对于每个表征，我们都可以计算 $Q(REPR_S, A)$ , 最终得到 $M$ 个 $Q$ ，将结果平均即可实现 $Q(S,do(A))$ 的计算）\n",
    "\n",
    "\n",
    "\n",
    "### 1.4 注意事项\n",
    "\n",
    "**提交作业时请注意**：\n",
    "\n",
    "提交作业时请导入必要的包和第三方库 (包括此文件中曾经导入过的)。\n",
    "\n",
    "对于任务1：\n",
    "\n",
    "a. 请你补充 `main.ipynb`的代码, 设计策略模型 `class AC1`, 将`task_name` 参数设置为 `1`, 并进行训练，\n",
    "\n",
    "b. 模型会保存在 `checkpoint/1/ac_params.pth` ，请勿修改模型和文件夹名称\n",
    "\n",
    "对于任务2：\n",
    "\n",
    "a. 请你完成生成  `main.ipynb`的代码, 设计策略模型 `class AC1`, 将`task_name` 参数设置为 `2-1` 或者`2-2`, 并进行训练.\n",
    "\n",
    "b. 模型会保存在 `checkpoint/2-1/ac_params.pth` 或者 `checkpoint/2-2/ac_params.pth`，请自行创建文件夹，请勿修改模型和文件夹名称\n",
    "\n",
    "**作业评分程序会自动加载模型并对测试结果进行评分。**\n",
    "**请在训练之前将`main.ipynb`转为 python 文件 `main.py`， 否则评分程序无法进行打分。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.评分标准\n",
    "任务1：评分程序会自动加载保存的模型，并测试 32 个 episode，\n",
    "1. `coding_here.ipynb`可以运行, 并可以保存`checkpoint`, 得 10 分\n",
    "2. 测试平均duration 大于20, 得60分。\n",
    "3. 测试平均duration 大于8, 得30分。\n",
    "\n",
    "任务2-1：评分程序会自动加载保存的模型，并测试 32 个 episode，\n",
    "1. 测试平均duration 大于20, 且平均duration大于task1, 得15分。\n",
    "2. 测试平均duration 大于15, 且平均duration大于task1, 得10分。\n",
    "3. 测试平均duration 大于8, 得5分。\n",
    "\n",
    "任务2-2：评分程序会自动加载保存的模型，并测试 32 个 episode，\n",
    "1. 测试平均duration 大于20, 且平均duration大于task1, 得30分。\n",
    "2. 测试平均duration 大于20,  得20分。\n",
    "3. 测试平均duration 大于15,  得15分。\n",
    "4. 测试平均duration 大于8, 得10分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch implementation of Actor Critic\n",
    "import os\n",
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "import pdb\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch Policy Graident Actor Critic example at openai-gym pong')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n",
    "                    help='discount factor (default: 0.99')\n",
    "parser.add_argument('--decay_rate', type=float, default=0.99, metavar='G',\n",
    "                    help='decay rate for RMSprop (default: 0.99)')\n",
    "parser.add_argument('--learning_rate', type=float, default=3e-4, metavar='G',\n",
    "                    help='learning rate (default: 1e-4)')\n",
    "parser.add_argument('--batch_size', type=int, default=32, metavar='G',\n",
    "                    help='Every how many episodes to da a param update')\n",
    "parser.add_argument('--seed', type=int, default=87, metavar='N',\n",
    "                    help='random seed (default: 87)')\n",
    "\n",
    "# 请根据所做任务修改此参数\n",
    "parser.add_argument('--task_name', type=str, default='1', \n",
    "                    help='任务名为 1, 2-1, 2-2') \n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "torch.manual_seed(args.seed)\n",
    "task_name = args.task_name\n",
    "ck_path = 'checkpoint/{}/ac_params.pth'.format(task_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**请在此设计 task1 和 task2 的网络结构，请勿修改类名**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 请设计 task1 和 task2 的网络结构，请勿修改类名\n",
    "class AC1(nn.Module):\n",
    "    def __init__(self, num_actions=1):\n",
    "        super(AC1, self).__init__()\n",
    "        # 请自定义网络结构\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 请自定义网络前馈传播计算，返回action, 和价值函数state_values，维度都为(batch_size, 1), action 取值在(0~1)之间\n",
    "\n",
    "        return action, state_values\n",
    "    \n",
    "    \n",
    "class AC2(nn.Module):\n",
    "    def __init__(self, num_actions=1):\n",
    "        super(AC2, self).__init__()\n",
    "        # 请自定义网络结构\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 请自定义网络前馈传播计算，返回action, 和价值函数state_values，维度都为(batch_size, 1), action 取值在(0~1)之间\n",
    "\n",
    "        return action, state_values    \n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "torch.manual_seed(args.seed)\n",
    "task_name = args.task_name\n",
    "ck_path = 'checkpoint/{}/ac_params.pth'.format(task_name)\n",
    "# built policy network\n",
    "if task_name=='1':\n",
    "    policy = AC1()\n",
    "else:\n",
    "    policy = AC2()\n",
    "\n",
    "policy_rewards = []\n",
    "policy_saved_log_probs = []\n",
    "\n",
    "# construct a optimal function\n",
    "optimizer = optim.RMSprop(policy.parameters(), lr=args.learning_rate, weight_decay=args.decay_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**此函数根据模型以及状态选择合适的action，请勿修改**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(model, x):\n",
    "    x = Variable(torch.from_numpy(x).float().unsqueeze(0)) \n",
    "    m = Categorical(probs)\n",
    "    action = m.sample() # action.shape = torch.Size([1])    \n",
    "    policy_saved_log_probs[-1].append((m.log_prob(action), state_value))\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**此函数负责 epiosde roll out 以及 loss 的计算，请在此补齐相应的 value function，以及 loss 的计算**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    value_loss = []\n",
    "    rewards = [] \n",
    "    for episode_id, episode_reward_list in enumerate(policy_rewards): \n",
    "        for i, r in enumerate(episode_reward_list):\n",
    "            if i == len(episode_reward_list) - 1:\n",
    "                R = torch.scalar_tensor(r)\n",
    "            else:\n",
    "                R = r + args.gamma * policy_saved_log_probs[episode_id][i + 1][1] \n",
    "            rewards.append(R) \n",
    "    if is_cuda: \n",
    "        rewards = rewards.cuda()\n",
    "    flatten_log_probs = [sample for episode in policy_saved_log_probs for sample in episode]\n",
    "    assert len(flatten_log_probs) == len(rewards)\n",
    "\n",
    "   \n",
    "    for (log_prob, value), reward in zip(flatten_log_probs, rewards):\n",
    "        # 若实验为任务一，请补充value_func， 任务二步骤1，请补充advantage_func，任务二步骤2请补充do_value_func\n",
    "        # 不需要的变量请删除， 例任务一 value_func = reward\n",
    "        value_func=\n",
    "        advantage_func=\n",
    "        do_value_func=\n",
    "        \n",
    "        \n",
    "    # 请在此计算 policy_loss, value_loss  \n",
    "    \n",
    "    \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = policy_loss + value_loss\n",
    "    if is_cuda:\n",
    "        loss.cuda()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # clean rewards and saved_actions\n",
    "    del policy_rewards[:]\n",
    "    del policy_saved_log_probs[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**主函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop\n",
    "if __name__ == '__main__':\n",
    "    running_reward = None\n",
    "    reward_sum = 0\n",
    "\n",
    "    # for i_episode in count(1):\n",
    "    for i_episode in range(32):\n",
    "        state = env.reset()    \n",
    "        state = np.array(state[0])\n",
    "        prev_x = np.zeros(4) \n",
    "        policy_rewards.append([])  # record rewards separately for each episode\n",
    "        policy_saved_log_probs.append([])\n",
    "\n",
    "        for t in range(10000):\n",
    "            cur_x = state\n",
    "            x = cur_x - prev_x \n",
    "            prev_x = cur_x\n",
    "            action = select_action(policy,x) \n",
    "            state, reward, done, _ = env.step(action.cpu().numpy()[0])\n",
    "            state = np.array(state)\n",
    "            reward_sum += reward\n",
    "            policy_rewards[-1].append(reward) \n",
    "\n",
    "            if done:\n",
    "                # tracking log\n",
    "                running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "                print('Actor Critic ep %03d done. reward: %f. reward running mean: %f' % (i_episode, reward_sum, running_reward))\n",
    "                reward_sum = 0\n",
    "                break\n",
    "\n",
    "\n",
    "        # use policy gradient update model weights\n",
    "        # Every how many episodes to do a param update: batch_size = 20\n",
    "        if i_episode % args.batch_size == 0 : \n",
    "            finish_episode()\n",
    "\n",
    "        # Save model in every 50 episode\n",
    "        if i_episode % 50 == 0:\n",
    "            print('ep %d: model saving...' % (i_episode))\n",
    "            torch.save(policy,ck_path)\n",
    "\n",
    "    # 如果需要进行测试，和可视化实验结果请在此添加代码\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 请将以下代码补全后转化为 main.py文件提交"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5><b>提交的代码中只需要包括`AC1`和`AC2`两个类及其所用到的 Python 库</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "select": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "import pdb\n",
    "\n",
    "# 请设计 task1 和 task2 的网络结构，请勿修改类名\n",
    "class AC1(nn.Module):\n",
    "    def __init__(self, num_actions=1):\n",
    "        super(AC1, self).__init__()\n",
    "        # 请自定义网络结构\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 请自定义网络前馈传播计算，返回action, 和价值函数state_values，维度都为(batch_size, 1), action 取值在(0~1)之间\n",
    "\n",
    "        return action, state_values\n",
    "    \n",
    "    \n",
    "class AC2(nn.Module):\n",
    "    def __init__(self, num_actions=1):\n",
    "        super(AC2, self).__init__()\n",
    "        # 请自定义网络结构\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 请自定义网络前馈传播计算，返回action, 和价值函数state_values，维度都为(batch_size, 1), action 取值在(0~1)之间\n",
    "\n",
    "        return action, state_values  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "nbTranslate": {
   "displayLangs": [
    "fr",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
